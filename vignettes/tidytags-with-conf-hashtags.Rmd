---
title: "Using tidytags with a conference hashtag"
output: 
  rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Using tidytags with a conference hashtag}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(collapse = TRUE, comment = "#>", fig.show = "hide")
library(tidytags)
```

This vignette introduces how to use many **tidytags** functions through the example of analyzing tweets associated with the 2019 annual convention of the [Association for Educational Communications & Technology](https://aect.org/) (AECT): `#aect19`, `#aect2019`, or `#aect19inspired`.

Most of the information in this vignette is available scattered throughout the R documentation. This appendix brings it all together in one place.

## read_tags()

To simply view a TAGS archive, you can use `read_tags()`. Here, we've openly shared a TAGS tracker that has been collecting tweets associated with the AECT 2019 since September 30, 2019. This tracker is active through today.

Keep in mind that `read_tags()` uses the **googlesheets** package, and one requirement is that your TAGS has been published to the web. To do this, with the TAGS page open in a browser, go to `File` >> `Publish to the web`. The `Link` field should be 'Entire document' and the `Embed` field should be 'Web page.' If everything looks right, then click the `Publish` button. Next, click the `Share` button in the top right corner of the Google Sheets window, select `Get shareable link`, and set the permissions to 'Anyone with the link can view.' Now you should be ready to go.


```{r initial-tags, message = FALSE, warning = FALSE}
example_url <- "https://docs.google.com/spreadsheets/d/18clYlQeJOc6W5QRuSlJ6_v3snqKJImFhU42bRkM_OX8/edit#gid=8743918"

example_df_all <- read_tags(example_url)
dim(example_df_all)
```

## pull_tweet_data()

With TAGS imported into R, **tidytags** allows you to gather quite a bit more information related to the collected tweets with the `pull_tweet_data()` function. This function uses the [rtweet package](https://rtweet.info/) (via `rtweet::lookup_statuses()`) to query the Twitter API. Using **rtweet** requires a Twitter developer account; see the rtweet vignette [Obtaining and using access tokens](https://rtweet.info/articles/auth.html) as a guide to get started. 

Note that your dataset will often contain fewer rows after running `pull_tweet_data()`. This is because `rtweet::lookup_statuses()` is searching for tweet IDs that are currently available. Any tweets that have been deleted or made "protected" (i.e., private) since TAGS first collected them are dropped from the dataset. Rather than view this as a limitation, we see th asset to help ensure our data better reflects the intentions of the accounts whose tweets we have collected (see Fiesler & Proferes, 2018).

Here, we demonstrate two different ways of using `pull_tweet_data()`. The first method is to query the Twitter API with the tweet ID numbers from the `id_str` column returned by **rtweet**. However, a limitation of **TAGS** is that the numbers in this column are often corrupted because Google Sheets considers them very large numbers (instead of character strings) and rounds them by putting them into exponential form. The results of this first method are stored in the variable `example_after_rtweet_A` below. The second method pulls the tweet ID numbers from the tweet URLs. For example, the tweet with the URL `https://twitter.com/tweet__example/status/1176592704647716864` has a tweet ID of `1176592704647716864`. The results of this second method are stored in the variable `example_after_rtweet_B` below.

```{r method-comparison}
example_after_rtweet_A <- pull_tweet_data(example_df_all$id_str)
example_after_rtweet_B <- pull_tweet_data(get_char_tweet_ids(example_df_all))
```

```{r method-comparison-output, echo = FALSE}
paste0("The TAGS tracker alone collected ", length(names(example_df_all)), 
       " variables associated with ", nrow(example_df_all), " tweets."
       ); paste0("The first rtweet method searching with 'id_str' collected ", 
                 length(names(example_after_rtweet_A)), 
                 " variables associated with ", nrow(example_after_rtweet_A), " tweets."
       ); paste0("The second rtweet method using 'tidytags::get_char_tweet_ids()' collected ", 
                 length(names(example_after_rtweet_B)), 
                 " variables associated with ", nrow(example_after_rtweet_B), " tweets."
       )
```

Notice how many more variables are in the dataset after using `pull_tweet_data()`, and how many more tweets are in the dataset when using the second method. Therfore, we strongly recommend the second method, which is why we have included the `get_char_tweet_ids()` in the **tidytags** package. 

Now take a quick look at the best result, viewed with the `glimpse()` function from the **dplyr** package:

```{r glimpse}
example_after_rtweet <- example_after_rtweet_B
dplyr::glimpse(example_after_rtweet)
```

At this point, the purpose of the **tidytags** R package should be restated. **TAGS** tweet trackers are easily set up and maintained, and does an excellent job passively collecting tweets over time. For instance, the example TAGS tracker we demo here has collected thousands of tweets related to the AECT 2019 annual convention since September 30, 2019. In contrast, running this query now using `rtweet::search_tweets()` is limited by Twitter's API, meaning that an **rtweet** search can only go back in time 6-9 days, and is limited to returning at most 18,000 tweets per query. So, if you are interested in tweets about AECT 2019, today you could get almost no meaningful data using **rtweet** alone. 

```{r rtweet-today, message = FALSE}
rtweet_today <- rtweet::search_tweets("#AERA19 OR #AERA2019 OR #aect19inspired", n = 18000)
paste("Number of tweets returned by an rtweet search today:", nrow(rtweet_today))
```

In sum, although **TAGS** is great at easily collecting tweets over time (breadth), it lacks depth in terms of metadata is returned related to the gathered tweets. Specifically, **TAGS** returns information on at most 18 variables; in contrast, **rtweet** returns information on up to 90 variables. Thus our package **tidytags** is needed to combine the breadth of TAGS with the depth of rtweet.

## lookup_many_tweets()

The Twitter API only allows the looking up of 90,000 tweet IDs at a time, a rate limit which resets after 15 minutes. Hence `rtweet::lookup_statuses()` will only return results for the first 90,000 tweet IDs in your dataset. The function `tidytags::lookup_many_tweets()` will automotically break your dataset into batches of 90,000 tweets, looking up one batch per 15 minutes until finished. Note that `lookup_many_tweets()` also works for datasets with fewer than 90,000 tweets as well.

Because our AECT 2019 examples includes fewer than 90,000 tweets (and because `lookup_many_tweets()` involves waiting for 15 minutes between batches), we do not include an example here. However, this function can be used in the same way as `pull_tweet_data()`.

## process_tweets(), process_tweets_flattened()

After `pull_tweet_data()` is used to collect additional information from TAGS tweet IDS (in this case, the `example_after_rtweet` dataframe), **tidytags** functions `process_tweets()` or `process_tweets_flattened()` can be used to calculate additional attributes and add these to the dataframe as new columns. Specifically, 10 new variables are added: word_count, character_count, mentions_count, hashtags_count_api, hashtags_count_regex, has_hashtags, urls_count_api, urls_count_regex, is_reply, and is_self_reply. 

```{r process-tweets, warning = FALSE}
example_processed <- process_tweets(example_after_rtweet)
paste0("We now have ", length(names(example_processed)), " variables associated with ", 
       nrow(example_processed), " tweets.")
```

At this point, depending on your research questions, you may wish to calculate some descriptive statistics associated with this tweet data. For instance the mean number of characters per tweet:

```{r stats-char}
mean_char <- round(mean(example_processed$character_count), 2)
sd_char <- round(sd(example_processed$character_count), 2)

paste0("The mean number of characters per tweet is ", mean_char, 
       " (SD = ", sd_char, ")."
       )
```

Or, perhaps, the mean, median, and max number of hashtags per tweet:

```{r stats-hashtags}
mean_hash <- round(mean(example_processed$hashtags_count_regex), 2)
sd_hash <- round(sd(example_processed$hashtags_count_regex), 2)
median_hash <- median(example_processed$hashtags_count_regex)
max_hash <- max(example_processed$hashtags_count_regex)

paste0("The mean number of hashtags per tweet is ", mean_hash, 
       " (SD = ", sd_hash, "). The median is ", median_hash,
       " and the maximum number of hashtags in a tweet is ", max_hash, "."
       )
```

## get_url_domain()

It may also be of interest to examine what websites get linked to most often in your dataset. The **tidytags** function `get_url_domain()` combines the `expand_urls()` function from the **longurl** package and the `domain()` function from the **urltools** package to easily return the domain names of any hyperlinks including in tweets. Note that `longurl::expand_urls()` is necessary because Twitter automatically shortens any hyperlinks included in tweets. As an example:

```{r url-example}
short_url <- "http://bit.ly/2SfWO3K"
get_url_domain(short_url)
```

`get_url_domain()` can then be combined with a function like `dplyr::count()` to

Output a count table of domains present in the dataset

Keep in mind, this process is a bit slow.

```{r domain-table, message = FALSE, warning = FALSE}
example_urls <- purrr::flatten_chr(example_processed$urls_url)
example_urls <- example_urls[!is.na(example_urls)] # remove NA values
example_domains <- get_url_domain(example_urls)
domain_table <- as.data.frame(table(example_domains))
table_sorted <- dplyr::arrange(domain_table, desc(Freq))
head(table_sorted, 20)
```

## geocode_tags()

Another area to explore is where tweeters in the dataset are from (or, at least, where they self-identify in their Twitter profiles). **tidytags** makes this straightforward with the `geocode_tags()` function. Note that `geocode_tags()` should be used after additional metadata has been retrieved with `tidytags::pull_tweet_data()`.

`geocode_tags()` pulls from the Google Geocoding API, which requires a Google Geocoding API Key. You can easily secure a key through Google Cloud Platform; [read more here](https://developers.google.com/maps/documentation/geocoding/get-api-key). Next, we recommend saving your Google Geocoding API Key in the .Renviron file as **'Google_API_key'**. You can quickly access this file using the R code `usethis::edit_r_environ(scope='user')`.

```{r edit-r-environ, eval=FALSE}
usethis::edit_r_environ(scope='user')
```

Once you've saved this file, quit your R session and restart. The function `tidytags::geocode_tags()` will work for you from now on. 

We've paired `tidytags::geocode_tags()` with the **mapview** package to allow for quick, inteactive viewing of the geocoded data. Read more about mapview [here](https://r-spatial.github.io/mapview/).

```{r, message=FALSE}
example_geo_coords <- geocode_tags(example_after_rtweet)
example_map <- mapview::mapview(example_geo_coords$pnt) 
mapview::mapshot(example_map, file="example-map.png")
example_map
```

## get_upstream_replies()

```{r}
sample1000 <- pull_tweet_data(get_char_tweet_ids(dplyr::sample_n(example_df_all, 1000)))
sample_with_upstream <- get_upstream_replies(sample1000)
dim(sample1000); dim(sample_with_upstream)
```

## get_replies(), get_retweets(), get_quotes(), get_mentions()

```{r}
nrow(get_replies(example_after_rtweet)); nrow(get_retweets(example_after_rtweet)); nrow(get_quotes(example_after_rtweet)); nrow(get_mentions(example_after_rtweet))
```

## create_edgelist()

Create an edgelist from the TAGS data using the `tidytags::create_edgelist()` function:

```{r}
example_edgelist <- create_edgelist(example_after_rtweet)
head(example_edgelist, 20)
dplyr::count(example_edgelist, edge_type)
```

## add_users_data()

**tidytags** also has functionality to add user-level data to an edgelist through the function `tidytags::add_users_data()`.

```{r}
example_users_data <- rtweet::users_data(example_after_rtweet)
example_senders_receivers_data <- add_users_data(example_edgelist, example_users_data)
dplyr::glimpse(example_senders_receivers_data)
```
