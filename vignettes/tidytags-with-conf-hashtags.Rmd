---
title: "Using tidytags with a conference hashtag"
output: 
  rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Using tidytags with a conference hashtag}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(collapse = TRUE, comment = "#>", fig.show = "hide")
library(tidytags)
```

This vignette introduces how to use many **tidytags** functions through the example of analyzing tweets associated with the 2019 annual convention of the [Association for Educational Communications & Technology](https://aect.org/) (AECT): `#aect19`, `#aect2019`, or `#aect19inspired`.

Most of the information in this vignette is available scattered throughout the R documentation. This appendix brings it all together in one place.

## read_tags()

To simply view a TAGS archive, you can use `read_tags()`. Here, we've openly shared a TAGS tracker that has been collecting tweets associated with the AECT 2019 since September 30, 2019. This tracker is active through today.

Keep in mind that `read_tags()` uses the **googlesheets** package, and one requirement is that your TAGS has been published to the web. To do this, with the TAGS page open in a browser, go to `File` >> `Publish to the web`. The `Link` field should be 'Entire document' and the `Embed` field should be 'Web page.' If everything looks right, then click the `Publish` button. Next, click the `Share` button in the top right corner of the Google Sheets window, select `Get shareable link`, and set the permissions to 'Anyone with the link can view.' Now you should be ready to go.


```{r initial-tags, message = FALSE, warning = FALSE}
example_url <- "https://docs.google.com/spreadsheets/d/18clYlQeJOc6W5QRuSlJ6_v3snqKJImFhU42bRkM_OX8/edit#gid=8743918"

example_df_all <- read_tags(example_url)
dim(example_df_all)
```

## pull_tweet_data()

With TAGS imported into R, **tidytags** allows you to gather quite a bit more information related to the collected tweets with the `pull_tweet_data()` function. This function uses the [rtweet package](https://rtweet.info/) (via `rtweet::lookup_statuses()`) to query the Twitter API. Using **rtweet** requires a Twitter developer account; see the rtweet vignette [Obtaining and using access tokens](https://rtweet.info/articles/auth.html) as a guide to get started. 

Note that your dataset will often contain fewer rows after running `pull_tweet_data()`. This is because `rtweet::lookup_statuses()` is searching for tweet IDs that are currently available. Any tweets that have been deleted or made "protected" (i.e., private) since TAGS first collected them are dropped from the dataset. Rather than view this as a limitation, we see th asset to help ensure our data better reflects the intentions of the accounts whose tweets we have collected (see Fiesler & Proferes, 2018).

Here, we demonstrate two different ways of using `pull_tweet_data()`. The first method is to query the Twitter API with the tweet ID numbers from the `id_str` column returned by **rtweet**. However, a limitation of **TAGS** is that the numbers in this column are often corrupted because Google Sheets considers them very large numbers (instead of character strings) and rounds them by putting them into exponential form. The results of this first method are stored in the variable `example_after_rtweet_A` below. The second method pulls the tweet ID numbers from the tweet URLs. For example, the tweet with the URL `https://twitter.com/tweet__example/status/1176592704647716864` has a tweet ID of `1176592704647716864`. The results of this second method are stored in the variable `example_after_rtweet_B` below.

```{r method-comparison}
example_after_rtweet_A <- pull_tweet_data(example_df_all$id_str)
example_after_rtweet_B <- pull_tweet_data(get_char_tweet_ids(example_df_all))
```

```{r method-comparison-output, echo = FALSE}
paste0("The TAGS tracker alone collected ", length(names(example_df_all)), 
       " variables associated with ", nrow(example_df_all), " tweets."
       ); paste0("The first rtweet method searching with 'id_str' collected ", 
                 length(names(example_after_rtweet_A)), 
                 " variables associated with ", nrow(example_after_rtweet_A), " tweets."
       ); paste0("The second rtweet method using 'tidytags::get_char_tweet_ids()' collected ", 
                 length(names(example_after_rtweet_B)), 
                 " variables associated with ", nrow(example_after_rtweet_B), " tweets."
       )
```

Notice how many more variables are in the dataset after using `pull_tweet_data()`, and how many more tweets are in the dataset when using the second method. Therfore, we strongly recommend the second method, which is why we have included the `get_char_tweet_ids()` in the **tidytags** package. 

Now take a quick look at the best result, viewed with the `glimpse()` function from the **dplyr** package:

```{r glimpse}
example_after_rtweet <- example_after_rtweet_B
dplyr::glimpse(example_after_rtweet)
```

At this point, the purpose of the **tidytags** R package should be restated. **TAGS** tweet trackers are easily set up and maintained, and does an excellent job passively collecting tweets over time. For instance, the example TAGS tracker we demo here has collected thousands of tweets related to the AECT 2019 annual convention since September 30, 2019. In contrast, running this query now using `rtweet::search_tweets()` is limited by Twitter's API, meaning that an **rtweet** search can only go back in time 6-9 days, and is limited to returning at most 18,000 tweets per query. So, if you are interested in tweets about AECT 2019, today you could get almost no meaningful data using **rtweet** alone. 

```{r rtweet-today, message = FALSE}
rtweet_today <- rtweet::search_tweets("#AERA19 OR #AERA2019 OR #aect19inspired", n = 18000)
paste("Number of tweets returned by an rtweet search today:", nrow(rtweet_today))
```

In sum, although **TAGS** is great at easily collecting tweets over time (breadth), it lacks depth in terms of metadata is returned related to the gathered tweets. Specifically, **TAGS** returns information on at most 18 variables; in contrast, **rtweet** returns information on up to 90 variables. Thus our package **tidytags** is needed to combine the breadth of TAGS with the depth of rtweet.

## lookup_many_tweets()

The Twitter API only allows the looking up of 90,000 tweet IDs at a time, a rate limit which resets after 15 minutes. Hence `rtweet::lookup_statuses()` will only return results for the first 90,000 tweet IDs in your dataset. The function `tidytags::lookup_many_tweets()` will automotically break your dataset into batches of 90,000 tweets, looking up one batch per 15 minutes until finished. Note that `lookup_many_tweets()` also works for datasets with fewer than 90,000 tweets as well.

Because our AECT 2019 examples includes fewer than 90,000 tweets (and because `lookup_many_tweets()` involves waiting for 15 minutes between batches), we do not include an example here. However, this function can be used in the same way as `pull_tweet_data()`.

## process_tweets(), process_tweets_flattened()

After `pull_tweet_data()` is used to collect additional information from TAGS tweet IDS (in this case, the `example_after_rtweet` dataframe), **tidytags** functions `process_tweets()` or `process_tweets_flattened()` can be used to calculate additional attributes and add these to the dataframe as new columns. Specifically, 10 new variables are added: word_count, character_count, mentions_count, hashtags_count_api, hashtags_count_regex, has_hashtags, urls_count_api, urls_count_regex, is_reply, and is_self_reply. 

```{r process-tweets, warning = FALSE}
example_processed <- process_tweets(example_after_rtweet)
paste0("We now have ", length(names(example_processed)), " variables associated with ", 
       nrow(example_processed), " tweets.")
```

At this point, depending on your research questions, you may wish to calculate some descriptive statistics associated with this tweet data. For instance the mean number of characters per tweet:

```{r stats-char}
mean_char <- round(mean(example_processed$character_count), 2)
sd_char <- round(sd(example_processed$character_count), 2)

paste0("The mean number of characters per tweet is ", mean_char, 
       " (SD = ", sd_char, ")."
       )
```

Or, perhaps, the mean, median, and max number of hashtags per tweet:

```{r stats-hashtags}
mean_hash <- round(mean(example_processed$hashtags_count_regex), 2)
sd_hash <- round(sd(example_processed$hashtags_count_regex), 2)
median_hash <- median(example_processed$hashtags_count_regex)
max_hash <- max(example_processed$hashtags_count_regex)

paste0("The mean number of hashtags per tweet is ", mean_hash, 
       " (SD = ", sd_hash, "). The median is ", median_hash,
       " and the maximum number of hashtags in a tweet is ", max_hash, "."
       )
```

## get_url_domain()

The **tidytags** function `get_url_domain()` combines the `expand_urls()` function from the **longurl** package and the `domain()` function from the **urltools** package to easily return the domain names of any hyperlinks including in tweets. Note that using `longurl::expand_urls()` is a necessary step because Twitter automatically shortens any hyperlinks included in tweets. As an example:

```{r url-example}
short_url <- "http://bit.ly/2SfWO3K"
get_url_domain(short_url)
```

It may also be of interest to examine what websites get linked to most often in your dataset. `get_url_domain()` can be combined with a function like `table()` to calculate frequency counts for domains present in the dataset. This process is useful to get a picture of to where else on the Internet tweeters are directing their readers' attention.

Keep in mind, however, that this process is a bit slow.

```{r domain-table, message = FALSE, warning = FALSE}
example_urls <- purrr::flatten_chr(example_processed$urls_url)
example_urls <- example_urls[!is.na(example_urls)] # Remove NA values
example_domains <- get_url_domain(example_urls)
domain_table <- as.data.frame(table(example_domains))
table_sorted <- dplyr::arrange(domain_table, desc(Freq))
head(table_sorted, 20)
```

## geocode_tags()

Another area to explore is where tweeters in the dataset are from (or, at least, the location they self-identify in their Twitter profiles). **tidytags** makes this straightforward with the `geocode_tags()` function. Note that `geocode_tags()` should be used after additional metadata has been retrieved with `tidytags::pull_tweet_data()`.

`geocode_tags()` pulls from the Google Geocoding API, which requires a Google Geocoding API Key. You can easily secure a key through Google Cloud Platform; [read more here](https://developers.google.com/maps/documentation/geocoding/get-api-key). Next, we recommend saving your Google Geocoding API Key in the .Renviron file as **'Google_API_key'**. You can quickly access this file using the R code `usethis::edit_r_environ(scope='user')`.

```{r edit-r-environ, eval=FALSE}
usethis::edit_r_environ(scope='user')
```

Once you've saved this file, quit your R session and restart. The function `tidytags::geocode_tags()` will work for you from now on. 

You can pair `geocode_tags()` with the **mapview** package to allow for quick, inteactive viewing of the geocoded data; read more about mapview [here](https://r-spatial.github.io/mapview/). There are many additional R packages that can plot coordinates on a map; which you choose is largely a matter of personal preference.

```{r map, message = FALSE, warning = FALSE}
example_unique_places <- dplyr::distinct(example_processed, location, .keep_all = TRUE)
example_geo_coords <- geocode_tags(example_unique_places)
example_map <- mapview::mapview(example_geo_coords$pnt) 
example_map
```

Finally, if you want to save a screenshot of the map, you can use the function `mapview::mapshot()`.

```{r mapshot, eval = FALSE}
mapview::mapshot(example_map, file="example-map.png") # Saves a screenshot of the map
```

## get_replies(), get_retweets(), get_quotes(), get_mentions()

These functions quickly subset the data, returning just the tweets of the type indicated by the function name (e.g., `get_replies()` returns only reply tweets). The `get_` family of functions can be used to look at home many tweets of each type are present in the dataset.

```{r tweet-types}
paste0("In the dataset of ", nrow(example_processed), " tweets, there are ", 
       nrow(get_replies(example_processed)), " replies, ",
       nrow(get_retweets(example_processed)), " retweets, ",
       nrow(get_quotes(example_processed)), " quote tweets, and ",
       nrow(get_mentions(example_processed)), " mentions."
       )
```

## get_upstream_replies()

If your research questions conceptualizing your tweet dataset as a conversation or affinity space, it may be useful to retrieve and add additional tweets. Specifically, TAGS collects tweets that contain one or more keywords or text strings. For example, the TAGS tracker we have been working with in this vignette collected tweets containing the keywords: `#aect19` OR `#aect2019` OR `#aect19inspired`. This is a reasonable approach, from a researchers' point of view. However, participants who have been following or contributing to these hashtags would also see additional tweets in these "conversations" because Twitter connects together tweets that reply to other tweets into potentially lengthy *reply threads*. Tweets in a reply thread are all displayed to a user viewing tweets on Twitter's platform, but because some tweets in a thread may not contain the hashtag of interest, not all tweets in the users' experience of a conversation would be collected by TAGS. Additionally, tweets contained in a reply thread but composed before the TAGS tracker was initiated would also be left out of the dataset.

There is a solution to this problem. Because the Twitter API offers a `reply_to_status_id` column, it is possible to iteratively reconstruct reply threads in an *upstream* direction, that is, retrieving tweets composed earlier than replies in the dataset. We include the `get_upstream_replies` in **tidytags** to streamline this process. We also print output at each iteration to demonstrate how the process is progressing.

```{r upstream-replies}
example_with_upstream <- get_upstream_replies(example_processed)
paste0("The dataset contained ", nrow(example_processed), " tweets at the start. ", 
       "Runnning 'get_upstream_replies()' added ", nrow(example_with_upstream) - nrow(example_processed),
       " new tweets.")
```

Unfortunately, it is not practical to retrieve *downstream* replies, or those tweets in a reply thread that follow a tweet in the dataset but neglect to include the hashtag or keyword.

## create_edgelist()

Another usefuyl approach to social media resarch is *social network analysis*. Getting started with social network analysis is as simple as producing an *edgelist*, a two-column dataframe listing *senders* and *receivers*. An edgelist gives a complete accounting of whom is interacting with whom. In Twitter, this is complicated somewhat by the number of ways a user is able to interact with someone else: namely, through replying, retweeting, quote tweeting, mentioning, and liking tweets. The **tidytags** function `create_edgelist()` uses `get_replies()`, `get_retweets()`, `get_quotes()`, and `get_mentions()` to create an edgelist that takes into account these four different types of interaction. `create_edgelist()` returns a dataframe with three columns: two for the sender and receiver Twitter handles, and a third column listing the edge type (i.e., the form of interaction).

```{r edgelist}
example_edgelist <- create_edgelist(example_processed)
head(example_edgelist, 20)
```

Running `create_edgelist()` also provides a simple way to re-look at home many tweets of each type are present in the dataset, using the `count()` function from **dplyr**.

```{r edge-table}
dplyr::count(example_edgelist, edge_type, sort = TRUE)
```

Note that we do not yet have a function `get_likes()` because this process is much more difficult given the information provided by the Twitter API.

## add_users_data()

Finally, **tidytags** also has functionality to add user-level data to an edgelist through the function `tidytags::add_users_data()`. These additional features are very useful when taking an inferential approach to social network analysis, such as building *influence* or *selection* models.

```{r user-data}
example_users_data <- rtweet::users_data(example_processed)
example_senders_receivers_data <- add_users_data(example_edgelist, example_users_data)
dplyr::glimpse(example_senders_receivers_data)
```

```{r session}
devtools::session_info()
```
